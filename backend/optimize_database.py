#!/usr/bin/env python3
"""
ChronoRetrace - æ•°æ®åº“ä¼˜åŒ–ç®¡ç†è„šæœ¬

æœ¬è„šæœ¬æä¾›æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–çš„ç»Ÿä¸€å…¥å£ï¼ŒåŒ…æ‹¬ç´¢å¼•ä¼˜åŒ–ã€è¿ç§»æ‰§è¡Œç­‰åŠŸèƒ½ã€‚
å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ‰§è¡Œä¸åŒçš„ä¼˜åŒ–æ“ä½œã€‚

Author: ChronoRetrace Team
Date: 2024
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from app.infrastructure.database.index_optimization import (
    get_database_optimization_status,
    optimize_database_indexes,
)
from app.infrastructure.database.migrations import (
    get_database_migration_status,
    run_database_migrations,
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("database_optimization.log"),
    ],
)

logger = logging.getLogger(__name__)


def optimize_indexes():
    """
    æ‰§è¡Œæ•°æ®åº“ç´¢å¼•ä¼˜åŒ–
    """
    logger.info("å¼€å§‹æ‰§è¡Œæ•°æ®åº“ç´¢å¼•ä¼˜åŒ–...")

    try:
        result = optimize_database_indexes()

        print("\n=== æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–ç»“æœ ===")
        print(f"å¼€å§‹æ—¶é—´: {result['start_time']}")
        print(f"ç»“æŸæ—¶é—´: {result['end_time']}")
        print(f"æ€»è€—æ—¶: {result['duration_seconds']:.2f} ç§’")
        print(f"å®Œæˆæ­¥éª¤: {', '.join(result['steps_completed'])}")

        if result.get("index_results"):
            print("\nç´¢å¼•åˆ›å»ºç»“æœ:")
            for index_name, success in result["index_results"].items():
                status = "âœ“ æˆåŠŸ" if success else "- è·³è¿‡"
                print(f"  {index_name}: {status}")

        if result.get("errors"):
            print("\né”™è¯¯ä¿¡æ¯:")
            for error in result["errors"]:
                print(f"  âŒ {error}")

        logger.info("æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–å®Œæˆ")
        return True

    except Exception as e:
        logger.error(f"æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–å¤±è´¥: {e}")
        print(f"\nâŒ ä¼˜åŒ–å¤±è´¥: {e}")
        return False


def run_migrations(target_version=None):
    """
    æ‰§è¡Œæ•°æ®åº“è¿ç§»

    Args:
        target_version: ç›®æ ‡ç‰ˆæœ¬
    """
    logger.info(f"å¼€å§‹æ‰§è¡Œæ•°æ®åº“è¿ç§»åˆ°ç‰ˆæœ¬: {target_version or 'æœ€æ–°'}")

    try:
        result = run_database_migrations(target_version)

        print("\n=== æ•°æ®åº“è¿ç§»ç»“æœ ===")
        print(f"å¼€å§‹æ—¶é—´: {result['start_time']}")
        print(f"ç»“æŸæ—¶é—´: {result.get('end_time', 'æœªå®Œæˆ')}")
        print(f"æ€»è€—æ—¶: {result['total_execution_time_ms']} ms")
        print(f"æˆåŠŸ: {'æ˜¯' if result['success'] else 'å¦'}")

        if result["applied_migrations"]:
            print("\nå·²åº”ç”¨çš„è¿ç§»:")
            for migration in result["applied_migrations"]:
                print(
                    f"  âœ“ {migration['version']}: {migration['name']} ({migration['execution_time_ms']}ms)"
                )
        else:
            print("\næ²¡æœ‰éœ€è¦åº”ç”¨çš„è¿ç§»")

        if result.get("errors"):
            print("\né”™è¯¯ä¿¡æ¯:")
            for error in result["errors"]:
                print(f"  âŒ {error}")

        logger.info("æ•°æ®åº“è¿ç§»å®Œæˆ")
        return result["success"]

    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿ç§»å¤±è´¥: {e}")
        print(f"\nâŒ è¿ç§»å¤±è´¥: {e}")
        return False


def show_status():
    """
    æ˜¾ç¤ºæ•°æ®åº“ä¼˜åŒ–çŠ¶æ€
    """
    logger.info("è·å–æ•°æ®åº“ä¼˜åŒ–çŠ¶æ€...")

    try:
        # è·å–ä¼˜åŒ–çŠ¶æ€
        opt_status = get_database_optimization_status()

        # è·å–è¿ç§»çŠ¶æ€
        migration_status = get_database_migration_status()

        print("\n=== æ•°æ®åº“ä¼˜åŒ–çŠ¶æ€ ===")

        # æ˜¾ç¤ºè¡¨ç»Ÿè®¡ä¿¡æ¯
        if (
            "query_analysis" in opt_status
            and "table_stats" in opt_status["query_analysis"]
        ):
            print("\nè¡¨ç»Ÿè®¡ä¿¡æ¯:")
            for table, stats in opt_status["query_analysis"]["table_stats"].items():
                print(
                    f"  {table}: {stats['total_records']:,} æ¡è®°å½• (~{stats['estimated_size_mb']:.1f} MB)"
                )

        # æ˜¾ç¤ºç°æœ‰ç´¢å¼•
        if (
            "index_usage" in opt_status
            and "existing_indexes" in opt_status["index_usage"]
        ):
            print(
                f"\nç°æœ‰ç´¢å¼• ({len(opt_status['index_usage']['existing_indexes'])} ä¸ª):"
            )
            for index in opt_status["index_usage"]["existing_indexes"]:
                print(f"  â€¢ {index['name']} (è¡¨: {index['table']})")

        # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
        if "recommendations" in opt_status:
            print("\nä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(opt_status["recommendations"], 1):
                print(f"  {i}. {rec}")

        # æ˜¾ç¤ºè¿ç§»çŠ¶æ€
        print("\n=== æ•°æ®åº“è¿ç§»çŠ¶æ€ ===")

        if "applied_migrations" in migration_status:
            print(f"\nå·²åº”ç”¨çš„è¿ç§» ({len(migration_status['applied_migrations'])} ä¸ª):")
            for migration in migration_status["applied_migrations"]:
                print(
                    f"  âœ“ {migration['version']}: {migration['name']} (åº”ç”¨äº: {migration['applied_at']})"
                )

        if "pending_migrations" in migration_status:
            if migration_status["pending_migrations"]:
                print(
                    f"\nå¾…æ‰§è¡Œçš„è¿ç§» ({len(migration_status['pending_migrations'])} ä¸ª):"
                )
                for migration in migration_status["pending_migrations"]:
                    print(
                        f"  â³ {migration['version']}: {migration['name']} - {migration['description']}"
                    )
            else:
                print("\nâœ“ æ‰€æœ‰è¿ç§»éƒ½å·²åº”ç”¨")

        logger.info("çŠ¶æ€è·å–å®Œæˆ")
        return True

    except Exception as e:
        logger.error(f"è·å–çŠ¶æ€å¤±è´¥: {e}")
        print(f"\nâŒ è·å–çŠ¶æ€å¤±è´¥: {e}")
        return False


def full_optimization():
    """
    æ‰§è¡Œå®Œæ•´çš„æ•°æ®åº“ä¼˜åŒ–æµç¨‹
    """
    logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´çš„æ•°æ®åº“ä¼˜åŒ–æµç¨‹...")

    print("\nğŸš€ å¼€å§‹å®Œæ•´æ•°æ®åº“ä¼˜åŒ–æµç¨‹")
    print("=" * 50)

    success = True

    # æ­¥éª¤1: æ‰§è¡Œè¿ç§»
    print("\nğŸ“‹ æ­¥éª¤ 1/2: æ‰§è¡Œæ•°æ®åº“è¿ç§»")
    if not run_migrations():
        success = False
        print("âŒ è¿ç§»å¤±è´¥ï¼Œåœæ­¢ä¼˜åŒ–æµç¨‹")
        return False

    # æ­¥éª¤2: ä¼˜åŒ–ç´¢å¼•
    print("\nğŸ”§ æ­¥éª¤ 2/2: ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•")
    if not optimize_indexes():
        success = False
        print("âš ï¸ ç´¢å¼•ä¼˜åŒ–å¤±è´¥ï¼Œä½†è¿ç§»å·²å®Œæˆ")

    if success:
        print("\nğŸ‰ æ•°æ®åº“ä¼˜åŒ–æµç¨‹å…¨éƒ¨å®Œæˆï¼")
        print("\nå»ºè®®æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ä¼˜åŒ–æ•ˆæœ:")
        print("  python optimize_database.py --status")
    else:
        print("\nâš ï¸ ä¼˜åŒ–æµç¨‹éƒ¨åˆ†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

    return success


def export_status(output_file):
    """
    å¯¼å‡ºæ•°æ®åº“çŠ¶æ€åˆ°æ–‡ä»¶

    Args:
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    logger.info(f"å¯¼å‡ºæ•°æ®åº“çŠ¶æ€åˆ°æ–‡ä»¶: {output_file}")

    try:
        # è·å–çŠ¶æ€ä¿¡æ¯
        opt_status = get_database_optimization_status()
        migration_status = get_database_migration_status()

        # åˆå¹¶çŠ¶æ€ä¿¡æ¯
        full_status = {
            "timestamp": datetime.utcnow().isoformat(),
            "optimization_status": opt_status,
            "migration_status": migration_status,
        }

        # å†™å…¥æ–‡ä»¶
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(full_status, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nâœ“ çŠ¶æ€å·²å¯¼å‡ºåˆ°: {output_file}")
        logger.info(f"çŠ¶æ€å¯¼å‡ºå®Œæˆ: {output_file}")
        return True

    except Exception as e:
        logger.error(f"çŠ¶æ€å¯¼å‡ºå¤±è´¥: {e}")
        print(f"\nâŒ å¯¼å‡ºå¤±è´¥: {e}")
        return False


def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(
        description="ChronoRetrace æ•°æ®åº“ä¼˜åŒ–ç®¡ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python optimize_database.py --status                    # æŸ¥çœ‹å½“å‰çŠ¶æ€
  python optimize_database.py --optimize                  # æ‰§è¡Œå®Œæ•´ä¼˜åŒ–
  python optimize_database.py --indexes                   # ä»…ä¼˜åŒ–ç´¢å¼•
  python optimize_database.py --migrate                   # æ‰§è¡Œè¿ç§»
  python optimize_database.py --migrate --version 001     # è¿ç§»åˆ°æŒ‡å®šç‰ˆæœ¬
  python optimize_database.py --export status.json       # å¯¼å‡ºçŠ¶æ€åˆ°æ–‡ä»¶
        """,
    )

    parser.add_argument("--status", action="store_true", help="æ˜¾ç¤ºæ•°æ®åº“ä¼˜åŒ–çŠ¶æ€")
    parser.add_argument(
        "--optimize", action="store_true", help="æ‰§è¡Œå®Œæ•´çš„æ•°æ®åº“ä¼˜åŒ–æµç¨‹"
    )
    parser.add_argument("--indexes", action="store_true", help="ä»…æ‰§è¡Œç´¢å¼•ä¼˜åŒ–")
    parser.add_argument("--migrate", action="store_true", help="æ‰§è¡Œæ•°æ®åº“è¿ç§»")
    parser.add_argument("--version", type=str, help="æŒ‡å®šè¿ç§»ç›®æ ‡ç‰ˆæœ¬")
    parser.add_argument("--export", type=str, metavar="FILE", help="å¯¼å‡ºçŠ¶æ€åˆ°æŒ‡å®šæ–‡ä»¶")
    parser.add_argument("--verbose", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # æ‰§è¡Œç›¸åº”æ“ä½œ
    success = True

    if args.status:
        success = show_status()
    elif args.optimize:
        success = full_optimization()
    elif args.indexes:
        success = optimize_indexes()
    elif args.migrate:
        success = run_migrations(args.version)
    elif args.export:
        success = export_status(args.export)
    else:
        parser.print_help()
        return 0

    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
